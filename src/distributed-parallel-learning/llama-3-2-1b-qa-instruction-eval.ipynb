{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2-4fmBm3eEUu",
        "outputId": "091130b4-59ff-4618-f4be-60020d408389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: jinja2 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: networkx in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: filelock in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: fsspec in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: numpy in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: transformers==4.51.3 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from transformers==4.51.3) (3.13.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from transformers==4.51.3) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from transformers==4.51.3) (4.67.1)\n",
            "Requirement already satisfied: requests in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from transformers==4.51.3) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from transformers==4.51.3) (0.21.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from transformers==4.51.3) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from transformers==4.51.3) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from transformers==4.51.3) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from transformers==4.51.3) (2024.11.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from transformers==4.51.3) (0.30.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (4.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (2024.6.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from requests->transformers==4.51.3) (3.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from requests->transformers==4.51.3) (3.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from requests->transformers==4.51.3) (2025.6.15)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (from requests->transformers==4.51.3) (2.5.0)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install 'transformers==4.51.3'\n",
        "!pip install 'tqdm==4.67.1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wV_2TXekf5tX"
      },
      "outputs": [],
      "source": [
        "# Set Environment Variables\n",
        "default_environment_variables = {\n",
        "    \"model_directory\": \"./output/llama-3-2-1b-alpaca-202506241720/save_model\",\n",
        "    \"model_name\": \"meta-llama/Llama-3.2-1B\",\n",
        "    \"dataset\": \"../dataset/alpaca_data.json\",\n",
        "    \"apply_dataset\": 100\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/python_project/sample/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h-yY9gzgfGPT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-06-26 22:55:30,738] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        }
      ],
      "source": [
        "# load model, tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(default_environment_variables[\"model_directory\"])\n",
        "model = AutoModelForCausalLM.from_pretrained(default_environment_variables[\"model_directory\"], device_map='cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# change model mode to evaluation\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h2TFosyUmOkG"
      },
      "outputs": [],
      "source": [
        "# read dataset\n",
        "import pandas\n",
        "import json\n",
        "\n",
        "def load_alpaca_dataset():\n",
        "  with open(default_environment_variables[\"dataset\"], 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "  dataframe = pandas.DataFrame(data)\n",
        "  dataframe = dataframe[['instruction', 'input', 'output']]\n",
        "  dataframe.head(100)\n",
        "  return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vTzHrRXarBof"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                         instruction input  \\\n",
            "0               Give three tips for staying healthy.         \n",
            "1                 What are the three primary colors?         \n",
            "2                 Describe the structure of an atom.         \n",
            "3                   How can we reduce air pollution?         \n",
            "4  Describe a time when you had to make a difficu...         \n",
            "\n",
            "                                              output  \n",
            "0  1.Eat a balanced diet and make sure to include...  \n",
            "1  The three primary colors are red, blue, and ye...  \n",
            "2  An atom is made up of a nucleus, which contain...  \n",
            "3  There are a number of ways to reduce air pollu...  \n",
            "4  I had to make a difficult decision when I was ...  \n",
            "dataframe: 52002\n"
          ]
        }
      ],
      "source": [
        "dataframe = load_alpaca_dataset()\n",
        "print(dataframe.head())\n",
        "print(f\"dataframe: {len(dataframe)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                         instruction input  \\\n",
            "0               Give three tips for staying healthy.         \n",
            "1                 What are the three primary colors?         \n",
            "2                 Describe the structure of an atom.         \n",
            "3                   How can we reduce air pollution?         \n",
            "4  Describe a time when you had to make a difficu...         \n",
            "\n",
            "                                              output  \n",
            "0  1.Eat a balanced diet and make sure to include...  \n",
            "1  The three primary colors are red, blue, and ye...  \n",
            "2  An atom is made up of a nucleus, which contain...  \n",
            "3  There are a number of ways to reduce air pollu...  \n",
            "4  I had to make a difficult decision when I was ...  \n",
            "convert_dataset: 101\n"
          ]
        }
      ],
      "source": [
        "# split dataset & reset index\n",
        "splited_dataset = dataframe.loc[:default_environment_variables[\"apply_dataset\"]]\n",
        "convert_dataset = splited_dataset.reset_index(drop=True)\n",
        "print(convert_dataset.head())\n",
        "print(f\"convert_dataset: {len(convert_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "###instrruction:\n",
            "please answer\n",
            "\n",
            "###input:\n",
            "Hello.\n",
            "\n",
            "###output:\n",
            "Hello. How do you call the most interesting situation? Tell me from your friend? Please have to\n"
          ]
        }
      ],
      "source": [
        "# model inference\n",
        "def generate(mdoel, tokenizer, prompt, max_new_token):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda:0')\n",
        "    output = model.generate(**input_ids, pad_token_id=tokenizer.eos_token_id, temperature=0.9)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True, max_new_tokens=max_new_token)\n",
        "\n",
        "prompt = \"\"\"\n",
        "###instrruction:\n",
        "please answer\n",
        "\n",
        "###input:\n",
        "Hello.\n",
        "\n",
        "###output:\n",
        "\"\"\"\n",
        "answer = generate(model, tokenizer, prompt, max_new_token=512)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "import pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define blue\n",
        "def compute_bleu(predictions: list[str], references: list[list[str]]) -> dict[str, int | float | list[float]]:\n",
        "    \"\"\"BLUEを算出\"\"\"\n",
        "    # BLUEを算出する\n",
        "    result = corpus_bleu(predictions, references)\n",
        "    return {\n",
        "        \"score\": result.score,\n",
        "        \"counts\": result.counts,\n",
        "        \"totals\": result.totals,\n",
        "        \"precisions\": [round(p, 2) for p in result.precisions],\n",
        "        \"bp\": result.bp,\n",
        "        \"sys_len\": result.sys_len,\n",
        "        \"ref_len\": result.ref_len,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple JSON evaluation start...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 101/101 [00:23<00:00,  4.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluation completed JSON log saved to 'qa-evaluation.json'\n",
            "success rate: 101/101 (100.00%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# simple JSON evaluation\n",
        "\n",
        "def evaluate_generation_quality_simple_json(model, tokenizer, dataframe, num_samples=5):\n",
        "    # evaluation results\n",
        "    evaluation_results = []\n",
        "    \n",
        "    successful_generations = 0\n",
        "    \n",
        "    for i in tqdm(range(min(num_samples, len(dataframe)))):\n",
        "        try:\n",
        "            row = dataframe.iloc[i]\n",
        "            instruction = row['instruction']\n",
        "            input_text = row['input'] if row['input'] else \"\"\n",
        "            expected_output = row['output']\n",
        "            \n",
        "            # prompt creation\n",
        "            if input_text:\n",
        "                prompt = f\"###instruction:\\n{instruction}\\n###input:\\n{input_text}\\n###output:\\n\"\n",
        "            else:\n",
        "                prompt = f\"###instruction:\\n{instruction}\\n###output:\\n\"\n",
        "            \n",
        "            # generation execution\n",
        "            full_generated = generate(model, tokenizer, prompt, max_new_token=512)\n",
        "            \n",
        "            # model output only\n",
        "            generated_only = full_generated.replace(prompt, \"\").strip()\n",
        "            \n",
        "            successful_generations += 1\n",
        "\n",
        "            bleu_result = compute_bleu([generated_only], [[expected_output]])\n",
        "            \n",
        "            # JSON record creation\n",
        "            sample_result = {\n",
        "                \"sample\": i + 1,\n",
        "                \"instruction\": instruction,\n",
        "                \"expected\": expected_output,\n",
        "                \"generated\": generated_only,\n",
        "                \"bleu_score\": bleu_result[\"score\"],\n",
        "                \"bleu_counts\": bleu_result[\"counts\"],\n",
        "                \"bleu_totals\": bleu_result[\"totals\"],\n",
        "                \"bleu_precisions\": bleu_result[\"precisions\"],\n",
        "                \"bleu_bp\": bleu_result[\"bp\"],\n",
        "                \"bleu_sys_len\": bleu_result[\"sys_len\"],\n",
        "                \"bleu_ref_len\": bleu_result[\"ref_len\"]\n",
        "            }\n",
        "            \n",
        "            evaluation_results.append(sample_result)\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_result = {\n",
        "                \"sample\": i + 1,\n",
        "                \"instruction\": instruction if 'instruction' in locals() else \"N/A\",\n",
        "                \"expected\": \"N/A\",\n",
        "                \"generated\": f\"ERROR: {str(e)}\",\n",
        "                \"bleu_score\": \"N/A\",\n",
        "                \"bleu_counts\": \"N/A\",\n",
        "                \"bleu_totals\": \"N/A\",\n",
        "                \"bleu_precisions\": \"N/A\",\n",
        "                \"bleu_bp\": \"N/A\",\n",
        "                \"bleu_sys_len\": \"N/A\",\n",
        "                \"bleu_ref_len\": \"N/A\"\n",
        "            }\n",
        "            evaluation_results.append(error_result)\n",
        "            continue\n",
        "    \n",
        "    # JSON format file save\n",
        "    with open('qa-evaluation.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(evaluation_results, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"evaluation completed JSON log saved to 'qa-evaluation.json'\")\n",
        "    print(f\"success rate: {successful_generations}/{num_samples} ({successful_generations/num_samples*100:.2f}%)\")\n",
        "    \n",
        "    return evaluation_results\n",
        "\n",
        "# execution\n",
        "print(\"Simple JSON evaluation start...\")\n",
        "# evaluation_results = evaluate_generation_quality_simple_json(model, tokenizer, dataframe, num_samples=len(convert_dataset))\n",
        "evaluation_results = evaluate_generation_quality_simple_json(model, tokenizer, dataframe, num_samples=len(convert_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# 通常通りJSONを生成\n",
        "json_str = json.dumps(evaluation_results, ensure_ascii=False, indent=2)\n",
        "\n",
        "# 特定の配列を一行にする\n",
        "json_str = re.sub(r'(\"bleu_counts\"): \\[\\s*([^\\]]*)\\s*\\]', \n",
        "                  r'\\1: [\\2]', json_str, flags=re.MULTILINE | re.DOTALL)\n",
        "json_str = re.sub(r'(\"bleu_totals\"): \\[\\s*([^\\]]*)\\s*\\]', \n",
        "                  r'\\1: [\\2]', json_str, flags=re.MULTILINE | re.DOTALL)\n",
        "json_str = re.sub(r'(\"bleu_precisions\"): \\[\\s*([^\\]]*)\\s*\\]', \n",
        "                  r'\\1: [\\2]', json_str, flags=re.MULTILINE | re.DOTALL)\n",
        "\n",
        "# 配列内の改行とスペースを削除\n",
        "json_str = re.sub(r'\\[\\s*(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\s*\\]', \n",
        "                  r'[\\1, \\2, \\3, \\4]', json_str)\n",
        "json_str = re.sub(r'\\[\\s*([0-9.]+),\\s*([0-9.]+),\\s*([0-9.]+),\\s*([0-9.]+)\\s*\\]', \n",
        "                  r'[\\1, \\2, \\3, \\4]', json_str)\n",
        "\n",
        "# ファイルに保存\n",
        "with open('qa-evaluation.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json_str)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
